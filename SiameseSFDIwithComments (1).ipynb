{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ccd28b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relevant libraries and functions\n",
    "from __future__ import print_function\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.ndimage\n",
    "import numpy as np, h5py\n",
    "import os, time, sys\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.models import Model\n",
    "from keras.layers import BatchNormalization, Convolution2D, Input, SpatialDropout2D, UpSampling2D, MaxPooling2D, concatenate\n",
    "from keras.layers.core import Activation, Layer\n",
    "from keras.layers import Dense, Dropout, Conv1D, Input, Conv2D, add, Conv3D, Reshape\n",
    "from keras.callbacks import History, EarlyStopping, ModelCheckpoint, CSVLogger\n",
    "from itertools import cycle\n",
    "from sklearn import metrics\n",
    "import skimage\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "#from keras.optimizers import RMSprop\n",
    "from keras.optimizer_v2 import rmsprop\n",
    "#from keras.optimizer_v2 import RMSprop\n",
    "#from tensorflow.keras.optimizers import RMSprop\n",
    "from keras.utils import np_utils\n",
    "# from keras.backend.tensorflow_backend import set_session\n",
    "from keras.backend import set_session\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D, SeparableConv2D, Conv2DTranspose\n",
    "import pandas as pd\n",
    "import scipy.io\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential, load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb51259a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'), PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "#check physical devices, look for GPU\n",
    "print(tf.config.experimental.list_physical_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "04b5e28f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import training data\n",
    "\n",
    "#f_data = '.../trainData' # Directory with trainging data\n",
    "f_data = r\"C:\\Users\\micha\\Documents\\OneDrive - UHN\\Projects\\Students\\Arjun\\ImageData\\DL\\20211126_IcebergCase1Fixed\\trainingData\"\n",
    "stacks = os.listdir(f_data)\n",
    "numS = int(len(stacks))\n",
    "\n",
    "nF = 6 # Number of time-points\n",
    "xX = 81\n",
    "yY = 81\n",
    "nOP = 2 #2 optical properties\n",
    "\n",
    "sfdiD = np.ndarray(\n",
    "        (numS,  int(xX), int(yY), int(nF), int(1)), dtype=np.float32\n",
    "        )\n",
    "\n",
    "OP = np.ndarray(\n",
    "        (numS,  int(xX), int(yY),int(nOP)), dtype=np.float32\n",
    "        )\n",
    "depth = np.ndarray(\n",
    "        (numS, int(xX), int(yY), int(1)), dtype=np.float32\n",
    "        )\n",
    "qF = np.ndarray(\n",
    "        (numS, int(xX), int(yY), int(1)), dtype=np.float32\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2c034574",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0;\n",
    "for d in stacks:\n",
    "    # Save values to respective mapping\n",
    "    f = scipy.io.loadmat(os.path.join(f_data,d)) \n",
    "    sfdiD[i,:,:,:,0] = f['F']\n",
    "    f = scipy.io.loadmat(os.path.join(f_data,d)) \n",
    "    OP[i,:,:,:] = f['OP']\n",
    "    f = scipy.io.loadmat(os.path.join(f_data,d)) \n",
    "    depth[i,:,:,0] = f['Depth']\n",
    "    f = scipy.io.loadmat(os.path.join(f_data,d)) \n",
    "    qF[i,:,:,0] = f['QF']\n",
    "    i = i + 1\n",
    "\n",
    "#scaling features to same order of magnitude as depth 0.xxx\n",
    "sfdiD *=10**4\n",
    "OP[:,:,:,0] *=100\n",
    "OP[:,:,:,1]*=1/10\n",
    "qF *= 10**5\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6a1cb8bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.40036947"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sfdiD.shape\n",
    "# # OPm.shape\n",
    "# # f_dat\n",
    "# sfdiD[100,:,:,:,0]\n",
    "# tR[1,:,:,0]\n",
    "# tR.shape\n",
    "# tR.shape\n",
    "# depth.shape\n",
    "# # sfdiD.shape\n",
    "# np.mean(OP[:,:,:,1])\n",
    "# # OP[:,:,:,1] *=10\n",
    "# sfdiD.shape\n",
    "# # np.mean(OP[:,:,:,1])\n",
    "# # fig = plt.figure(figsize=(10,10))\n",
    "# # ax1 = fig.add_subplot(2,2,1)\n",
    "# # # Predicted amplitude ratio\n",
    "# # p1 = ax1.imshow(OP[1], interpolation='nearest', vmin=.5, vmax=1.5)\n",
    "np.mean(qF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8709edec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relevant resblock functions (Keras API)\n",
    "def resblock_2D(num_filters, size_filter, x):\n",
    "    Fx = Conv2D(num_filters, size_filter, padding='same', activation=None)(x)\n",
    "    Fx = Activation('relu')(Fx)\n",
    "    Fx = Conv2D(num_filters, size_filter, padding='same', activation=None)(Fx)\n",
    "    output = add([Fx, x])\n",
    "    output = Activation('relu')(output)\n",
    "    return output\n",
    "\n",
    "def resblock_2D_BN(num_filters, size_filter, x):\n",
    "    Fx = Conv2D(num_filters, size_filter, padding='same', activation=None)(x)\n",
    "    Fx = BatchNormalization()(Fx)\n",
    "    Fx = Activation('relu')(Fx)\n",
    "    Fx = Conv2D(num_filters, size_filter, padding='same', activation=None)(Fx)\n",
    "    Fx = BatchNormalization()(Fx)\n",
    "    output = add([Fx, x])\n",
    "    #output = BatchNormalization()(output)\n",
    "    output = Activation('relu')(output)\n",
    "    return output\n",
    "\n",
    "def resblock_3D_BN(num_filters, size_filter, x):\n",
    "    Fx = Conv3D(num_filters, size_filter, padding='same', activation=None)(x)\n",
    "    Fx = BatchNormalization()(Fx)\n",
    "    Fx = Activation('relu')(Fx)\n",
    "    Fx = Conv3D(num_filters, size_filter, padding='same', activation=None)(Fx)\n",
    "    Fx = BatchNormalization()(Fx)\n",
    "    output = add([Fx, x])\n",
    "    #output = BatchNormalization()(output)\n",
    "    output = Activation('relu')(output)\n",
    "    return output\n",
    "\n",
    "def xCeptionblock_2D_BN(num_filters, size_filter, x):\n",
    "    Fx = SeparableConv2D(num_filters, size_filter, padding='same', activation=None)(x)\n",
    "    Fx = BatchNormalization()(Fx)\n",
    "    Fx = Activation('relu')(Fx)\n",
    "    Fx = SeparableConv2D(num_filters, size_filter, padding='same', activation=None)(Fx)\n",
    "    Fx = BatchNormalization()(Fx)\n",
    "    output = add([Fx, x])\n",
    "    output = Activation('relu')(output)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "58aa95b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#preparing the model\n",
    "\n",
    "xX = 81;\n",
    "yY = 81;\n",
    "nF = 6;\n",
    "\n",
    "OpData = Input(shape=(xX,yY,2))\n",
    "OpProp = OpData\n",
    "t_data = Input(shape=(xX, yY, nF,1))\n",
    "sfdi = t_data\n",
    "\n",
    "OpProp = Conv2D(256, 1, padding='same', activation=None, data_format=\"channels_last\")(OpProp)\n",
    "OpProp = Conv2D(128, 1, padding='same', activation=None, data_format=\"channels_last\")(OpProp)\n",
    "OpProp = Conv2D(128, 1, padding='same', activation=None, data_format=\"channels_last\")(OpProp)\n",
    "\n",
    "#kernel size and stride size defined\n",
    "sfdi = Conv3D(50,kernel_size=(1,1,10),strides=(1,1,5), padding='same', activation=None, data_format=\"channels_last\")(sfdi)\n",
    "sfdi = BatchNormalization()(sfdi)\n",
    "sfdi = Activation('relu')(sfdi)\n",
    "sfdi = resblock_3D_BN(50, (1,1,5), sfdi)\n",
    "sfdi = Reshape((xX,yY,100))(sfdi)\n",
    "sfdi = Conv2D(256, 1, padding='same', activation=None, data_format=\"channels_last\")(sfdi)\n",
    "sfdi = Conv2D(256, 1, padding='same', activation=None, data_format=\"channels_last\")(sfdi)\n",
    "\n",
    "# concat = concatenate(axis=1)([OpProp, sfdi])\n",
    "concat = concatenate([OpProp,sfdi],axis=-1)\n",
    "concat = SeparableConv2D(512, 1, padding='same', activation=None, data_format=\"channels_last\")(concat)\n",
    "\n",
    "concat = resblock_2D_BN(512, 1, concat) #paper says 256 but it gives an error while 512 does not: \n",
    "# \"Operands could not be broadcast together with shapes (32, 32, 256) (32, 32, 512)\"\n",
    "concat = resblock_2D_BN(512, 1, concat) \n",
    "\n",
    "#concentration of fluorescence branch\n",
    "imgqF = Conv2D(64, 1, padding='same', activation=None)(concat)\n",
    "imgqF = BatchNormalization()(imgqF)\n",
    "imgqF = Activation('relu')(imgqF)\n",
    "imgqF = Conv2D(32, 1, padding='same', activation=None)(imgqF)\n",
    "imgqF = BatchNormalization()(imgqF)\n",
    "imgqF = Activation('relu')(imgqF)\n",
    "imgqF = Conv2D(1, 1, padding='same', activation=None)(imgqF)\n",
    "imgqF = Activation('relu')(imgqF)\n",
    "\n",
    "# depth Branch \n",
    "imgdepth = Conv2D(64, 1, padding='same', activation=None)(concat)\n",
    "imgdepth = BatchNormalization()(imgdepth)\n",
    "imgdepth = Activation('relu')(imgdepth)\n",
    "imgdepth = Conv2D(32, 1, padding='same', activation=None)(imgdepth)\n",
    "imgdepth = BatchNormalization()(imgdepth)\n",
    "imgdepth = Activation('relu')(imgdepth)\n",
    "imgdepth = Conv2D(1, 1, padding='same', activation=None)(imgdepth)\n",
    "imgdepth = Activation('relu')(imgdepth)\n",
    "\n",
    "\n",
    "\n",
    "#defining model inputs and outputs\n",
    "modelD = Model(inputs=[OpData,t_data], outputs=[imgqF, imgdepth])\n",
    "rmspropV = rmsprop.RMSprop(learning_rate=1e-5)\n",
    "\n",
    "modelD.compile(loss='mse',\n",
    "              optimizer=rmspropV,\n",
    "              metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a78c8843",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([None, 81, 81, 1])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imgqF.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "26bee148",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "1000/1000 [==============================] - 134s 126ms/step - loss: 0.1042 - activation_9_loss: 0.0385 - activation_12_loss: 0.0657 - activation_9_mae: 0.0290 - activation_12_mae: 0.0469 - val_loss: 0.0504 - val_activation_9_loss: 0.0084 - val_activation_12_loss: 0.0420 - val_activation_9_mae: 0.0183 - val_activation_12_mae: 0.0437\n",
      "Epoch 2/500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\micha\\anaconda3\\envs\\gtxDL\\lib\\site-packages\\keras\\utils\\generic_utils.py:494: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  warnings.warn('Custom mask layers require a config and must override '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 127s 127ms/step - loss: 0.0203 - activation_9_loss: 0.0027 - activation_12_loss: 0.0176 - activation_9_mae: 0.0112 - activation_12_mae: 0.0278 - val_loss: 0.1019 - val_activation_9_loss: 0.0060 - val_activation_12_loss: 0.0959 - val_activation_9_mae: 0.0182 - val_activation_12_mae: 0.0668\n",
      "Epoch 3/500\n",
      "1000/1000 [==============================] - 127s 127ms/step - loss: 0.0177 - activation_9_loss: 0.0017 - activation_12_loss: 0.0160 - activation_9_mae: 0.0088 - activation_12_mae: 0.0260 - val_loss: 0.0587 - val_activation_9_loss: 0.0030 - val_activation_12_loss: 0.0557 - val_activation_9_mae: 0.0126 - val_activation_12_mae: 0.0513\n",
      "Epoch 4/500\n",
      "1000/1000 [==============================] - 127s 127ms/step - loss: 0.0163 - activation_9_loss: 0.0012 - activation_12_loss: 0.0151 - activation_9_mae: 0.0075 - activation_12_mae: 0.0250 - val_loss: 0.0426 - val_activation_9_loss: 0.0059 - val_activation_12_loss: 0.0368 - val_activation_9_mae: 0.0185 - val_activation_12_mae: 0.0425\n",
      "Epoch 5/500\n",
      "1000/1000 [==============================] - 127s 127ms/step - loss: 0.0151 - activation_9_loss: 9.8221e-04 - activation_12_loss: 0.0141 - activation_9_mae: 0.0067 - activation_12_mae: 0.0240 - val_loss: 0.0363 - val_activation_9_loss: 0.0014 - val_activation_12_loss: 0.0349 - val_activation_9_mae: 0.0087 - val_activation_12_mae: 0.0407\n",
      "Epoch 6/500\n",
      "1000/1000 [==============================] - 127s 127ms/step - loss: 0.0147 - activation_9_loss: 8.3495e-04 - activation_12_loss: 0.0138 - activation_9_mae: 0.0062 - activation_12_mae: 0.0238 - val_loss: 0.0661 - val_activation_9_loss: 0.0070 - val_activation_12_loss: 0.0591 - val_activation_9_mae: 0.0146 - val_activation_12_mae: 0.0573\n",
      "Epoch 7/500\n",
      "1000/1000 [==============================] - 127s 127ms/step - loss: 0.0143 - activation_9_loss: 7.5005e-04 - activation_12_loss: 0.0136 - activation_9_mae: 0.0059 - activation_12_mae: 0.0236 - val_loss: 0.0426 - val_activation_9_loss: 0.0028 - val_activation_12_loss: 0.0398 - val_activation_9_mae: 0.0136 - val_activation_12_mae: 0.0449\n",
      "Epoch 8/500\n",
      "1000/1000 [==============================] - 127s 127ms/step - loss: 0.0139 - activation_9_loss: 6.4907e-04 - activation_12_loss: 0.0133 - activation_9_mae: 0.0055 - activation_12_mae: 0.0232 - val_loss: 0.0431 - val_activation_9_loss: 0.0038 - val_activation_12_loss: 0.0393 - val_activation_9_mae: 0.0156 - val_activation_12_mae: 0.0463\n",
      "Epoch 9/500\n",
      "1000/1000 [==============================] - 129s 129ms/step - loss: 0.0135 - activation_9_loss: 5.7855e-04 - activation_12_loss: 0.0129 - activation_9_mae: 0.0052 - activation_12_mae: 0.0228 - val_loss: 0.0508 - val_activation_9_loss: 0.0023 - val_activation_12_loss: 0.0485 - val_activation_9_mae: 0.0119 - val_activation_12_mae: 0.0486\n",
      "Epoch 10/500\n",
      "1000/1000 [==============================] - 131s 131ms/step - loss: 0.0132 - activation_9_loss: 5.2235e-04 - activation_12_loss: 0.0127 - activation_9_mae: 0.0049 - activation_12_mae: 0.0225 - val_loss: 0.0611 - val_activation_9_loss: 0.0075 - val_activation_12_loss: 0.0536 - val_activation_9_mae: 0.0161 - val_activation_12_mae: 0.0543\n",
      "Epoch 11/500\n",
      "1000/1000 [==============================] - 128s 128ms/step - loss: 0.0132 - activation_9_loss: 4.7225e-04 - activation_12_loss: 0.0127 - activation_9_mae: 0.0048 - activation_12_mae: 0.0223 - val_loss: 0.0522 - val_activation_9_loss: 0.0045 - val_activation_12_loss: 0.0477 - val_activation_9_mae: 0.0167 - val_activation_12_mae: 0.0504\n",
      "Epoch 12/500\n",
      "1000/1000 [==============================] - 127s 127ms/step - loss: 0.0126 - activation_9_loss: 4.4650e-04 - activation_12_loss: 0.0122 - activation_9_mae: 0.0045 - activation_12_mae: 0.0221 - val_loss: 0.0479 - val_activation_9_loss: 0.0055 - val_activation_12_loss: 0.0423 - val_activation_9_mae: 0.0145 - val_activation_12_mae: 0.0488\n",
      "Epoch 13/500\n",
      "1000/1000 [==============================] - 128s 128ms/step - loss: 0.0121 - activation_9_loss: 4.4074e-04 - activation_12_loss: 0.0117 - activation_9_mae: 0.0046 - activation_12_mae: 0.0214 - val_loss: 0.0609 - val_activation_9_loss: 0.0022 - val_activation_12_loss: 0.0587 - val_activation_9_mae: 0.0091 - val_activation_12_mae: 0.0541\n",
      "Epoch 14/500\n",
      "1000/1000 [==============================] - 127s 127ms/step - loss: 0.0122 - activation_9_loss: 3.9378e-04 - activation_12_loss: 0.0118 - activation_9_mae: 0.0043 - activation_12_mae: 0.0214 - val_loss: 0.0458 - val_activation_9_loss: 0.0044 - val_activation_12_loss: 0.0415 - val_activation_9_mae: 0.0106 - val_activation_12_mae: 0.0476\n",
      "Epoch 15/500\n",
      "1000/1000 [==============================] - 127s 127ms/step - loss: 0.0118 - activation_9_loss: 4.0330e-04 - activation_12_loss: 0.0114 - activation_9_mae: 0.0043 - activation_12_mae: 0.0210 - val_loss: 0.0588 - val_activation_9_loss: 0.0032 - val_activation_12_loss: 0.0557 - val_activation_9_mae: 0.0115 - val_activation_12_mae: 0.0552\n",
      "Epoch 16/500\n",
      "1000/1000 [==============================] - 129s 129ms/step - loss: 0.0119 - activation_9_loss: 3.4970e-04 - activation_12_loss: 0.0115 - activation_9_mae: 0.0041 - activation_12_mae: 0.0211 - val_loss: 0.0503 - val_activation_9_loss: 0.0092 - val_activation_12_loss: 0.0411 - val_activation_9_mae: 0.0202 - val_activation_12_mae: 0.0489\n",
      "Epoch 17/500\n",
      "1000/1000 [==============================] - 127s 127ms/step - loss: 0.0117 - activation_9_loss: 3.5971e-04 - activation_12_loss: 0.0114 - activation_9_mae: 0.0041 - activation_12_mae: 0.0208 - val_loss: 0.0449 - val_activation_9_loss: 0.0042 - val_activation_12_loss: 0.0407 - val_activation_9_mae: 0.0158 - val_activation_12_mae: 0.0489\n",
      "Epoch 18/500\n",
      "1000/1000 [==============================] - 127s 127ms/step - loss: 0.0114 - activation_9_loss: 3.2254e-04 - activation_12_loss: 0.0111 - activation_9_mae: 0.0039 - activation_12_mae: 0.0206 - val_loss: 0.0419 - val_activation_9_loss: 0.0038 - val_activation_12_loss: 0.0380 - val_activation_9_mae: 0.0161 - val_activation_12_mae: 0.0436\n",
      "Epoch 19/500\n",
      "1000/1000 [==============================] - 126s 126ms/step - loss: 0.0113 - activation_9_loss: 3.2662e-04 - activation_12_loss: 0.0110 - activation_9_mae: 0.0040 - activation_12_mae: 0.0204 - val_loss: 0.0425 - val_activation_9_loss: 0.0071 - val_activation_12_loss: 0.0354 - val_activation_9_mae: 0.0191 - val_activation_12_mae: 0.0437\n",
      "Epoch 20/500\n",
      "1000/1000 [==============================] - 125s 125ms/step - loss: 0.0111 - activation_9_loss: 3.3024e-04 - activation_12_loss: 0.0108 - activation_9_mae: 0.0039 - activation_12_mae: 0.0203 - val_loss: 0.0414 - val_activation_9_loss: 0.0029 - val_activation_12_loss: 0.0385 - val_activation_9_mae: 0.0103 - val_activation_12_mae: 0.0466\n"
     ]
    }
   ],
   "source": [
    "earlyStopping = EarlyStopping(monitor='val_loss', \n",
    "                              patience = 15, \n",
    "                              verbose = 0,\n",
    "                              mode = 'auto')\n",
    "\n",
    "fN = 'testName' # Assign some name for weights and training/validation loss curves here\n",
    "\n",
    "# Save loss curve (mse) and MAE information over all trained epochs. (monitor = '' can be changed to focus on other tau parameters)\n",
    "modelCheckPoint = ModelCheckpoint(filepath=fN+'.h5', \n",
    "                                  monitor='val_loss', \n",
    "                                  save_best_only=True, \n",
    "                                  verbose=0)\n",
    "# Train network (80/20 train/validation split, batch_size=20 recommended, nb_epoch may vary based on application)\n",
    "history = History()\n",
    "csv_logger = CSVLogger(fN+'.log')\n",
    "history = modelD.fit([OP, sfdiD], [qF, depth],\n",
    "          validation_split=0.2,\n",
    "          batch_size=4, epochs=500, verbose=1, shuffle=True, callbacks=[earlyStopping,csv_logger,modelCheckPoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a41bfd97",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelD.load_weights(fN+'.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c3e5285",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelD.save(\"modelDL2_10k_k116_s111.h5\") #save model to .h5 file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9794c129",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load model from .h5 file, make sure the model loaded corresponds to the defined model trained\n",
    "modelD.load_weights(\"modelDL2_10000.h5\")#10k_k113_s111.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f54408",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_data = r\"C:\\Users\\Arjun Jagota\\OneDrive - UHN\\Desktop\\SourceTreeGTx\\gtxDL\\DL4FLI-master\\FLINET_ex\\forwardSimulations\\DL2testData\"\n",
    "stacksT = os.listdir(t_data)\n",
    "numT = int(len(stacksT))\n",
    "\n",
    "nF = 6\n",
    "xX = 81\n",
    "yY = 81\n",
    "nOP = 2\n",
    "\n",
    "\n",
    "sfdiT = np.ndarray(\n",
    "        (numT, int(xX), int(yY), int(nF), int(1)), dtype=np.float32\n",
    "        )\n",
    "\n",
    "OPT = np.ndarray(\n",
    "        (numT, int(xX), int(yY), int(nOP)), dtype=np.float32\n",
    "        )\n",
    "depthT = np.ndarray(\n",
    "        (numT, int(xX), int(yY), int(1)), dtype=np.float32\n",
    "        )\n",
    "qFT = np.ndarray(\n",
    "        (numT, int(xX), int(yY), int(1)), dtype=np.float32\n",
    "        )\n",
    "\n",
    "    \n",
    "i = 0;\n",
    "for d in stacksT:\n",
    "    # Save values to respective mapping\n",
    "    f = scipy.io.loadmat(os.path.join(t_data,d)) \n",
    "    sfdiT[i,:,:,:,0] = f['F']\n",
    "    f = scipy.io.loadmat(os.path.join(t_data,d)) \n",
    "    OPT[i,:,:,:] = f['OP']\n",
    "    f = scipy.io.loadmat(os.path.join(t_data,d)) \n",
    "    depthT[i,:,:,0] = f['Depth']\n",
    "    f = scipy.io.loadmat(os.path.join(t_data,d)) \n",
    "    qFT[i,:,:,0] = f['QF']\n",
    "    i = i + 1    \n",
    "\n",
    "\n",
    "#Scaling of test data to be the same scale as trained data \n",
    "\n",
    "sfdiT *=10**4\n",
    "OPT[:,:,:,0] *=100\n",
    "OPT[:,:,:,1]*=1/10\n",
    "qFT *= 10**5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98058ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform inference on test data with trained model\n",
    "testV = modelD.predict([OPT, sfdiT])\n",
    "qFP = testV[0] # Predicted qF values\n",
    "depthP = testV[1] # Predicted depth values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f7bcf64",
   "metadata": {},
   "outputs": [],
   "source": [
    "#scaling fluoresence concentration for .mat file output in cell below\n",
    "qFPtrueScale = qFP*10**-5\n",
    "qFTtrueScale = qFT*10**-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a065afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving predicted data to .mat file \n",
    "scipy.io.savemat('PredictedFluorescenceQuantificationDL2_10000.mat',{\"quantifiedFluorPredicted\":qFPtrueScale})\n",
    "\n",
    "scipy.io.savemat('PredictedDepthDL2_10000.mat',{\"DepthPredicted\":depthP})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cff97ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Visualize example\n",
    "n = 57 # Number to illustrate w/ matplotlib\n",
    "fig = plt.figure(figsize=(15,15))\n",
    "ax1 = fig.add_subplot(2,2,1)\n",
    "# Predicted qF\n",
    "p1 = ax1.imshow(qFPtrueScale[n,:,:,0], interpolation='nearest', vmin=.0, vmax=10**-4)\n",
    "fig.colorbar(p1, ax=ax1)\n",
    "plt.title('Predicted Fluorescence Quantification')\n",
    "ax2 = fig.add_subplot(2,2,2)\n",
    "# G.T. quantified Fluorescence\n",
    "t1 = ax2.imshow(qFTtrueScale[n,:,:,0], interpolation='nearest', vmin=0, vmax=10**-4)\n",
    "fig.colorbar(t1, ax=ax2)\n",
    "plt.title('Ground Truth Fluorescence Quantification')\n",
    "ax3 = fig.add_subplot(2,2,3)\n",
    "# Predicted depth\n",
    "p2 = ax3.imshow(depthP[n,:,:,0], interpolation='nearest', vmin=0, vmax=5)\n",
    "fig.colorbar(p2, ax=ax3)\n",
    "plt.title('Predicted Depth (mm)')\n",
    "# G.T. depth\n",
    "ax4 = fig.add_subplot(2,2,4)\n",
    "t2 = ax4.imshow(depthT[n,:,:,0], interpolation='nearest', vmin=0, vmax=5)\n",
    "fig.colorbar(t2, ax=ax4)\n",
    "plt.title('Ground Truth Depth (mm)')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec5f930f",
   "metadata": {},
   "outputs": [],
   "source": [
    "n=32\n",
    "Diff = depthP[n,:,:,0]-depthT[n,:,:,0]\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "ax1 = fig.add_subplot(2,2,1)\n",
    "d1=ax1.imshow(Diff[:,:], interpolation='nearest', vmin=0, vmax=1)\n",
    "fig.colorbar(d1, ax=ax1)\n",
    "Diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d05ddef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#initializing matrix for SSIMs to be input into \n",
    "zErr = np.zeros((numT,2))\n",
    "tErr = np.zeros((numT,2))\n",
    "\n",
    "\n",
    "#iterating through each test data point to calculate SSIM \n",
    "for e in range(numT):\n",
    "    zErr[e,0] = ssim(depthP[e,:,:,0],depthT[e,:,:,0],multichannel=True)\n",
    "    nZ = np.unique(depthT[e,:,:,0])\n",
    "    np.nonzero(nZ)\n",
    "    zErr[e,1] = nZ[1]\n",
    "\n",
    "    \n",
    "\n",
    "for f in range(numT):\n",
    "    tErr[f,0] = ssim(qFP[f,:,:,0],qFT[f,:,:,0],multichannel=True)\n",
    "    nZ = np.unique(depthT[f,:,:,0])\n",
    "    np.nonzero(nZ)\n",
    "    tErr[f,1] = nZ[1]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af62a01a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#plotting depth SSIM vs depth, note this is with cylindrical tumour simulation, so constant depth\n",
    "plt.scatter(zErr[:,1],zErr[:,0])\n",
    "plt.xlabel('Depth (mm)')\n",
    "plt.ylabel('SSIM')\n",
    "plt.title('Depth vs SSIM of depth images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d987993",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting F conc. SSIM vs depth, note this is with cylindrical tumour simulation, so constant depth\n",
    "plt.scatter(tErr[:,1],tErr[:,0])\n",
    "plt.xlabel('Depth (mm)')\n",
    "plt.ylabel('SSIM')\n",
    "plt.title('Depth vs SSIM of qF images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d31fb156",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Mean of depth SSIM:\",np.mean(zErr[:,0]))\n",
    "print(\"Standard deviation of depth SSIM:\",np.std(zErr[:,0]))\n",
    "print(\"Mean of quantified Fluorescence SSIM:\",np.mean(tErr[:,0]))\n",
    "print(\"Standard deviation of quantified Fluorescence SSIM:\",np.std(tErr[:,0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ab6375",
   "metadata": {},
   "outputs": [],
   "source": [
    "ssim(depthP,depthT,multichannel=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b673199c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ssim(qFP,tRT,multichannel=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f833905",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c9daea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gtxDL",
   "language": "python",
   "name": "gtxdl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
